{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CS585_Homework2\n",
    "CS585 Image and Video Computing\n",
    "\n",
    "'''\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandGestureRecognition:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        # Convert to HSV color space for skin detection\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        lower_skin = np.array([0, 48, 80], dtype=np.uint8)\n",
    "        upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "        mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "\n",
    "        # Improved Morphological Operations\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        mask = cv2.erode(mask, kernel, iterations=2)\n",
    "        mask = cv2.dilate(mask, kernel, iterations=4)\n",
    "        mask = cv2.GaussianBlur(mask, (5, 5), 100)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if contours:\n",
    "            contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 5000]\n",
    "            if contours:\n",
    "                max_contour = max(contours, key=cv2.contourArea)\n",
    "                epsilon = 0.0005 * cv2.arcLength(max_contour, True)\n",
    "                approx = cv2.approxPolyDP(max_contour, epsilon, True)\n",
    "\n",
    "                cv2.drawContours(frame, [max_contour], -1, (0, 255, 0), 2)\n",
    "                cv2.drawContours(frame, [approx], -1, (255, 0, 0), 3)\n",
    "\n",
    "                hull = cv2.convexHull(max_contour, returnPoints=False)\n",
    "                if hull is not None and len(hull) > 3:\n",
    "                    defects = cv2.convexityDefects(max_contour, hull)\n",
    "                    if defects is not None:\n",
    "                        self.classify_gesture(defects, frame, max_contour)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def classify_gesture(self, defects, frame, contour):\n",
    "        count_defects = 0\n",
    "        for i in range(defects.shape[0]):\n",
    "            s, e, f, _ = defects[i][0]\n",
    "            start = tuple(contour[s][0])\n",
    "            end = tuple(contour[e][0])\n",
    "            far = tuple(contour[f][0])\n",
    "\n",
    "            a = np.sqrt((end[0] - start[0]) ** 2 + (end[1] - start[1]) ** 2)\n",
    "            b = np.sqrt((far[0] - start[0]) ** 2 + (far[1] - start[1]) ** 2)\n",
    "            c = np.sqrt((end[0] - far[0]) ** 2 + (end[1] - far[1]) ** 2)\n",
    "            angle = np.arccos((b ** 2 + c ** 2 - a ** 2) / (2 * b * c)) * (180 / np.pi)\n",
    "\n",
    "            if angle <= 90:\n",
    "                count_defects += 1\n",
    "\n",
    "        if count_defects == 0:\n",
    "            cv2.putText(frame, \"Closed Fist\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        elif count_defects == 1:\n",
    "            cv2.putText(frame, \"Peace sign\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        elif count_defects == 2:\n",
    "            cv2.putText(frame, \"Rock Star sign\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        elif count_defects == 3:\n",
    "            cv2.putText(frame, \"Okay sign\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        elif count_defects >= 4:\n",
    "            cv2.putText(frame, \"Open Fist\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    hand_gesture_recognition = HandGestureRecognition()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = hand_gesture_recognition.process_frame(frame)\n",
    "        cv2.imshow('Hand Gesture Recognition using Convex Hull', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show closed fist gesture to capture template image.\n",
      "closed fist template captured.\n",
      "Show open fist gesture to capture template image.\n",
      "open fist template captured.\n",
      "Show okay sign gesture to capture template image.\n",
      "okay sign template captured.\n",
      "Show peace sign gesture to capture template image.\n",
      "peace sign template captured.\n",
      "Show rockstar sign gesture to capture template image.\n",
      "rockstar sign template captured.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8e3deb38c5ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-8e3deb38c5ff>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhand_gesture_recognition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Hand Gesture Recognition with Template Matching'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-8e3deb38c5ff>\u001b[0m in \u001b[0;36mprocess_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mframe_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mtemplate_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;31m# Perform template matching\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class HandGestureRecognitionWithTemplateMatching:\n",
    "    def __init__(self):\n",
    "        self.template_images = {}\n",
    "\n",
    "    def capture_templates(self):\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        gesture_names = [\"closed_fist\", \"open_fist\", \"okay_sign\", \"peace_sign\", \"rockstar_sign\"]\n",
    "        x, y, w, h = 100, 100, 200, 200  # Define the coordinates and size of the box\n",
    "\n",
    "        for gesture_name in gesture_names:\n",
    "            print(f\"Show {gesture_name.replace('_', ' ')} gesture to capture template image.\")\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"Failed to capture frame.\")\n",
    "                    break\n",
    "\n",
    "                # Draw the rectangle on the frame\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                # Display message on the frame\n",
    "                cv2.putText(frame, f\"Capture {gesture_name.replace('_', ' ')}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                cv2.imshow('Hand Gesture Recognition', frame)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('c'):  # Press 'c' to capture template\n",
    "                    # Extract the region of interest (ROI)\n",
    "                    roi = frame[y:y+h, x:x+w]\n",
    "                    \n",
    "                    # Store the template image\n",
    "                    self.template_images[gesture_name] = roi.copy()\n",
    "                    print(f\"{gesture_name.replace('_', ' ')} template captured.\")\n",
    "                    break\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Save template images\n",
    "        for gesture_name, image in self.template_images.items():\n",
    "            cv2.imwrite(f\"{gesture_name}_template.jpg\", image)\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        predicted_gesture = None\n",
    "        max_matching_score = -1\n",
    "\n",
    "        for gesture_name, template_image in self.template_images.items():\n",
    "            # Convert to grayscale\n",
    "            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            template_gray = cv2.cvtColor(template_image, cv2.COLOR_BGR2GRAY)\n",
    "            x, y, w, h = 100, 100, 200, 200  # Define the coordinates and size of the box\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # Perform template matching\n",
    "            res = cv2.matchTemplate(frame_gray, template_gray, cv2.TM_CCOEFF_NORMED)\n",
    "            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "            if max_val > max_matching_score:\n",
    "                predicted_gesture = gesture_name\n",
    "                max_matching_score = max_val\n",
    "\n",
    "        if predicted_gesture:\n",
    "            # Display predicted gesture on the frame\n",
    "            cv2.putText(frame, f\"Predicted Gesture: {predicted_gesture.replace('_', ' ')}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        return frame\n",
    "\n",
    "def main():\n",
    "    # Create instance of HandGestureRecognitionWithTemplateMatching\n",
    "    hand_gesture_recognition = HandGestureRecognitionWithTemplateMatching()\n",
    "\n",
    "    # Capture template images\n",
    "    hand_gesture_recognition.capture_templates()\n",
    "\n",
    "    # Run hand gesture recognition with template matching\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = hand_gesture_recognition.process_frame(frame)\n",
    "        cv2.imshow('Hand Gesture Recognition with Template Matching', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
